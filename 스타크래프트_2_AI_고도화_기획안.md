# 스타크래프트 2 AI 고도화 기획안

**작성일**: 2026-01-17  
**프로젝트**: WickedZerg AI Bot  
**목적**: 저그 AI의 성능 향상을 위한 체계적인 연구 및 개발 계획

---

## 스타크래프트 2 AI (저그) 최적화 아이디어

저그는 **물량(Swarm)**과 맵 장악(Creep), 그리고 소모전이 핵심입니다. 프로게이머 리플레이를 학습하고 있다면, 아래 A~D 전략을 중심으로 최적화를 시도할 수 있습니다.

## ? 목차

1. [A. 마이크로 컨트롤: 군집 제어 (Boids Algorithm)](#a-마이크로-컨트롤-군집-제어-boids-algorithm)
2. [B. 매크로 판단: 계층적 강화학습 (Hierarchical RL)](#b-매크로-판단-계층적-강화학습-hierarchical-rl)
3. [C. 리플레이 학습의 한계 보완: 저그 특화 보상 설계 (Reward Shaping)](#c-리플레이-학습의-한계-보완-저그-특화-보상-설계-reward-shaping)
4. [D. 최신 트렌드: Transformer 기반 모델 (AlphaStar 방식)](#d-최신-트렌드-transformer-기반-모델-alphastar-방식)
5. [실현 가능성 및 단계별 구현 계획](#실현-가능성-및-단계별-구현-계획)

---

## 다음 단계 제안

방금 정리한 [A. 군집 제어](#a-마이크로-컨트롤-군집-제어-boids-algorithm), [B. 계층적 구조](#b-매크로-판단-계층적-강화학습-hierarchical-rl), [C. 보상 체계](#c-리플레이-학습의-한계-보완-저그-특화-보상-설계-reward-shaping), [D. 트랜스포머](#d-최신-트렌드-transformer-기반-모델-alphastar-방식) 4가지 핵심 전략은 **'AI 개발 기획서'**의 목차로 쓰기에 아주 훌륭합니다.

---

## A. 마이크로 컨트롤: 군집 제어 (Boids Algorithm)

### 배경 및 문제점

저그의 핵심인 **'저글링/뮤탈리스크'**는 개별 유닛보다 **뭉쳤을 때의 움직임**이 중요합니다.

현재의 문제:
- 단순히 적을 우클릭하는 수준의 제어
- 유닛들이 서로 겹치거나 비효율적으로 이동
- 적을 부드럽게 감싸는 고급 무빙 부재

### 해결 방안: Boids 알고리즘 적용

**Boids 알고리즘(분리, 정렬, 응집)**을 변형하여 적용:

#### 1. 분리 (Separation)
- 유닛들이 서로 겹치지 않도록 최소 거리 유지
- 충돌 방지 및 효율적인 공간 활용

#### 2. 정렬 (Alignment)
- 같은 방향으로 이동하는 유닛들의 속도 조정
- 군집의 일관된 움직임 유지

#### 3. 응집 (Cohesion)
- 유닛들이 중심점으로 모이도록 유도
- 적을 부드럽게 감싸는 형태의 무빙 구현

#### 4. 장점

- **학습 효과 극대화**: 드론 군집 비행 로직과 수학적으로 유사하여 학습 전이가 용이
- **자연스러운 무빙**: 생물학적 군집 행동을 모방하여 인간처럼 보이는 움직임
- **전투 효율 향상**: 유닛 손실 최소화 및 적 유닛 포위 효과 증대

#### 5. 수학적 모델

```
분리 벡터 (Separation) = Σ (현재 위치 - 인접 유닛 위치) / 거리²
정렬 벡터 (Alignment) = Σ (인접 유닛 속도 벡터) / 인접 유닛 수
응집 벡터 (Cohesion) = (인접 유닛 중심 위치 - 현재 위치)

최종 속도 = w1×분리 + w2×정렬 + w3×응집 + w4×목표 방향
```

**가중치 (w1, w2, w3, w4)**: 강화학습을 통해 최적화

---

## B. 매크로 판단: 계층적 강화학습 (Hierarchical RL)

### 배경 및 문제점

저그는 관리할 유닛과 건물이 많기 때문에, **하나의 두뇌(Agent)가 모든 걸 처리하면 과부하**가 걸립니다.

문제점:
- 판단해야 할 변수가 너무 많음 (자원, 인구수, 상대 종족, 전투 상태, 건물 상태, 등)
- 하나의 모델로 처리하면 학습이 매우 느림
- 전략과 전술이 혼재되어 의사결정이 비효율적

### 해결 방안: 계층적 구조 분리

#### 1. Commander Agent (사령관 - 상위 에이전트)

**역할**: 거시적 결정만 내림

**입력값**:
- 자원 (미네랄, 가스)
- 인구수 (현재/최대)
- 상대 종족
- 베이스 개수
- 현재 병력 규모

**출력 (전략 모드)**:
- `ECONOMY`: "지금은 드론을 째라" (경제 우선)
- `ALL_IN`: "올인 러시를 가라" (병력 집중 생산 및 공격)
- `DEFENSIVE`: "수비에 집중해라" (방어 건물, 유닛 생산)
- `TECH`: "테크를 올려라" (고급 유닛, 업그레이드)
- `TRANSITION`: "전환 단계" (경제에서 병력으로 등)

**결정 주기**: 30초~1분 단위

#### 2. Sub-Agents (하위 에이전트)

##### 2-1. Combat Agent (전투관)

**역할**: 사령관이 "공격해"라고 명령하면, 구체적인 전투 컨트롤만 담당

**담당 업무**:
- 유닛 산개 (Boids 알고리즘 활용)
- 점사 타겟 선택
- 위치 선정 (포위, 후퇴)

##### 2-2. Economy Agent (내정 에이전트)

**역할**: 건물 짓기, 자원 관리, 확장

**담당 업무**:
- 확장 타이밍
- 건물 배치
- 자원 할당

##### 2-3. Queen Agent (여왕 에이전트)

**역할**: 오직 **'펌핑(Inject Larva)'**과 **'점막(Creep Tumor)'** 생성 타이밍만 최적화

**담당 업무**:
- 라바 펌핑 타이밍 최적화
- 점막 종양 생성 위치 및 타이밍

#### 3. 구조 다이어그램

```
┌─────────────────────────────────────────┐
│    Commander Agent (Meta-Controller)    │
│    30초~1분마다 전략 모드 결정           │
│    - ECONOMY / ALL_IN / DEFENSIVE       │
└─────────────────────────────────────────┘
                    │
        ┌───────────┼───────────┐
        │           │           │
┌───────▼────┐ ┌───▼────┐ ┌───▼────┐
│  Combat    │ │Economy │ │ Queen  │
│  Agent     │ │ Agent  │ │ Agent  │
│ (전투)     │ │ (내정)  │ │ (펌핑)  │
└────────────┘ └────────┘ └────────┘
```

#### 4. 장점

- **책임 분리**: 각 에이전트가 자신의 전문 분야만 담당
- **학습 속도 향상**: 복잡한 문제를 작은 단위로 분해하여 학습 효율 증대
- **확장성**: 새로운 전략 모드나 하위 에이전트 추가가 용이

---

## C. 리플레이 학습의 한계 보완: 저그 특화 보상 설계 (Reward Shaping)

### 배경 및 문제점

**프로게이머 리플레이를 모방 학습(Imitation Learning)**하면 "비슷하게"는 하지만 **"왜" 하는지는 모를 수 있습니다.**

문제점:
- 단순 승리/패배 보상만으로는 학습이 매우 느림
- 게임이 끝날 때까지 수만 프레임을 허비
- 중간 과정에서 "잘하고 있다"는 피드백 부재

### 해결 방안: 세밀한 보상 체계 설계

#### 1. 저그 특화 보상 요소

##### 1-1. 점막 커버리지 보상 (맵 장악)

**목적**: 점막이 맵을 넓게 덮을수록 높은 보상

**설명**: 점막이 맵을 넓게 덮일수록 AI에게 높은 점수를 주어, 스스로 여왕을 생산하고 점막 종양(Creep Tumor)을 심도록 유도

**계산식**:
```
점막 커버리지 = (점막으로 덮인 타일 수) / (전체 맵 타일 수)
보상 = 점막 커버리지 × 5.0 (가중치 높음)
```

**효과**:
- 여왕 생산 유도
- 점막 종양(Creep Tumor) 생성 유도
- 시야 확보 및 이동 속도 버프 활용

##### 1-2. 라바 효율성 보상 (물량)

**목적**: 펌핑을 안 해서 라바가 쌓여있으면 감점

**계산식**:
```
각 해처리당 라바 개수 확인
if 라바 > 3:
    페널티 = -0.1 × (라바 개수 - 3)
```

**효과**:
- 펌핑 타이밍 최적화
- 라바 낭비 방지
- 물량 생산 효율 향상

##### 1-3. 자원 회전율 보상 (소모전)

**목적**: 미네랄이 2000 이상 남으면 '돈을 못 쓰고 있다'는 뜻이므로 감점

**계산식**:
```
if 미네랄 > 2000:
    페널티 = -0.05 × (미네랄 - 2000) / 1000
```

**효과**:
- 자원 낭비 방지
- 지속적인 유닛/건물 생산 유도
- 저그 특성 (돈을 남기면 집는다) 반영

##### 1-4. 전투 교전비 보상 (소모전 효율)

**목적**: (내가 파괴한 적 자원 가치) - (내가 잃은 자원 가치)의 변화량에 보상

**계산식**:
```
현재 교전비 = 파괴한 적 자원 가치 - 잃은 자원 가치
변화량 = 현재 교전비 - 이전 교전비
보상 = 변화량 × 0.001
```

**효과**:
- 전투 효율 향상
- 유닛 손실 최소화
- 적 유닛 파괴 최대화

#### 2. 보상 시스템 구조

```python
class ZergRewardSystem:
    def calculate_step_reward(self, bot) -> float:
        reward = 0.0
        
        # 1. 점막 커버리지 보상
        reward += self._calculate_creep_reward(bot)
        
        # 2. 라바 효율성 보상
        reward += self._calculate_larva_efficiency_reward(bot)
        
        # 3. 자원 회전율 보상
        reward += self._calculate_resource_turnover_reward(bot)
        
        # 4. 전투 교전비 보상
        reward += self._calculate_combat_exchange_reward(bot)
        
        return reward
```

#### 3. 장점

- **학습 속도 향상**: 중간 과정에서 지속적인 피드백 제공
- **저그 특성 반영**: 점막, 라바, 자원 회전율 등 저그만의 특징을 보상에 반영
- **전략 다양성**: 다양한 전략이 높은 보상을 받을 수 있도록 설계

---

## D. 최신 트렌드: Transformer 기반 모델 (AlphaStar 방식)

### 배경

현재 CNN/RNN 기반 모델의 한계:
- 시퀀스 데이터 처리에 제약
- 장기 의존성 학습 어려움

### 해결 방안: Transformer 구조 도입

#### 1. 게임 상태를 문장처럼 처리

**아이디어**: 게임의 상태(유닛 위치, 체력 등)를 문장처럼 처리하여 인과 관계를 더 잘 학습

**예시**:
```
"저글링이 체력이 없네 -> 뒤로 뺀다"
"적 유닛이 많네 -> 산개한다"
"점막이 부족하네 -> 점막 종양을 짓는다"
```

#### 2. Transformer의 장점

- **장기 의존성**: Attention 메커니즘으로 먼 과거의 정보도 활용
- **병렬 처리**: RNN보다 학습 속도가 빠름
- **계층적 구조**: 상위 레벨(전략)과 하위 레벨(전술)을 동시에 학습

#### 3. 구조 예시

```
게임 상태 → Embedding Layer
         ↓
Multi-Head Attention (N layers)
         ↓
Fully Connected Layer
         ↓
액션 출력 (전략 모드, 하위 액션)
```

---

## 실현 가능성 및 단계별 구현 계획

### 단계 1: 저그 특화 보상 시스템 (현재 진행 중 ?)

**우선순위**: 최우선  
**예상 기간**: 1주  
**난이도**: ?? (보통)

**구현 내용**:
- `ZergRewardSystem` 클래스 구현
- 4가지 보상 요소 통합
- 게임 루프에 보상 계산 연동

**현재 상태**: 
- ? 코드 구현 완료 (`local_training/reward_system.py`)
- ? 게임 루프 통합 필요

### 단계 2: 계층적 강화학습 구조 (현재 진행 중 ?)

**우선순위**: 높음  
**예상 기간**: 2주  
**난이도**: ??? (어려움)

**구현 내용**:
- `MetaController` 구현 (상위 에이전트)
- `SubControllers` 구현 (하위 에이전트들)
- 기존 매니저들과 통합

**현재 상태**:
- ? 기본 구조 구현 완료 (`local_training/hierarchical_rl/`)
- ? 기존 봇 코드와 통합 필요

### 단계 3: Boids 알고리즘 군집 제어 (예정)

**우선순위**: 중간  
**예상 기간**: 2주  
**난이도**: ??? (어려움)

**구현 내용**:
- Boids 알고리즘 구현 (분리, 정렬, 응집)
- 전투 모듈에 통합
- 강화학습을 통한 가중치 최적화

**현재 상태**:
- ? 설계 단계
- ? 구현 시작 전

### 단계 4: Transformer 기반 모델 (장기 계획)

**우선순위**: 낮음 (연구 단계)  
**예상 기간**: 1~2개월  
**난이도**: ????? (매우 어려움)

**구현 내용**:
- Transformer 구조 설계
- 기존 CNN/RNN 모델과 비교 실험
- AlphaStar 방식 참고

**현재 상태**:
- ? 연구 단계
- ? 논문 및 자료 조사 필요

---

## 결론

본 기획안에서 제시한 **4가지 핵심 전략**은 다음과 같은 순서로 구현할 것을 제안합니다:

1. ? **저그 특화 보상 시스템** (즉시 적용 가능, 효과 즉각적)
2. ? **계층적 강화학습 구조** (구조 개선, 장기적 효과)
3. ? **Boids 알고리즘 군집 제어** (전투 효율 향상)
4. ? **Transformer 기반 모델** (연구 단계, 장기 연구)

각 단계는 독립적으로 구현 가능하며, 점진적으로 통합하여 최종적으로 **고성능 저그 AI**를 완성할 수 있습니다.

---

## 참고 자료

- **Boids Algorithm**: Craig Reynolds, "Flocks, Herds, and Schools: A Distributed Behavioral Model" (1987)
- **Hierarchical RL**: Andrew G. Barto, "Hierarchical Learning in Stochastic Domains" (2003)
- **Reward Shaping**: Andrew Y. Ng, "Policy Invariance Under Reward Transformations" (1999)
- **AlphaStar**: DeepMind, "Grandmaster level in StarCraft II using multi-agent reinforcement learning" (2019)

---

**작성자**: WickedZerg AI Development Team  
**최종 수정일**: 2026-01-17
